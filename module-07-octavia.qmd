---
title: "Module 7: Octavia"
subtitle: "OS220: OpenStack Load Balancer as a Service"
format:
  revealjs:
    theme: default
    slide-number: true
    chalkboard: true
    preview-links: auto
    footer: "OS220 - OpenStack Administration"
    width: 1280
    height: 720
    margin: 0.1
    transition: slide
    background-transition: fade
  pptx:
    reference-doc: ./template.pptx
    highlight-style: tango
    slide-level: 2
footer: © Mirantis Training - OS220 - OpenStack Administration"

---

# Openstack Octavia {.center}

# (Lbaas V2 Service) {.center}

:::: {.columns}

::: {.column width="50%"}
![Slide 454 diagram](images/slide_454_image_01.png){width="90%"}
:::

::: {.column width="50%"}
![Slide 454 diagram](images/slide_454_image_02.png){width="90%"}
:::

::::

::: {.notes}
This next module covers OpenStack Octavia. Octavia is the OpenStack component that provides load balancing as a service (LBaaS). In particular, Octavia supports LBaaS v2, using HAproxy.
Octavia grew from the Neutron program.
The Octavia project is still under active development. As such there are updates to the most recent releases of OpenStack not covered in this course.
:::

---

At the end of this presentation, you should be able to:
Describe the Octavia architecture
Describe the function and purpose of the amphora Use the CLI to implement a load balancer solution

Objectives

455

::: {.notes}
At the end of this lesson, you should be able to:
    Describe the Octavia architecture;
    Describe the function and purpose of the amphora; and
    Use the CLI to implement a load balancer solution
During the lab exercise, you will use the CLI to create the various load balancer resources and the UI to verify your work.
Documentation on the Octavia CLI commands can be found at the provided URL.
    https://docs.openstack.org/python-octaviaclient/ussuri/cli/index.html
:::

---

# Octavia Lbaas V2 {.center}

# Load Balancer As A Service {.center}

::: {.notes}
This next section gives an overview of Octavia Load Balancer as a Service version 2.
:::

---

Application delivery, scaling, and availability are considered vital features of any cloud Load balancing is essential for enabling the applications
Octavia provides a reference implementation of OpenStack LBaaS v2
“Cloud-like” load balancing operation
Manage virtual machines to provide load balancing services

Overview

457

::: {.notes}
Octavia is an open source, operator-scale load balancing solution designed to work with OpenStack. Octavia was born out of the Neutron LBaaS project. Its conception influenced the transformation of the Neutron LBaaS project, as Neutron LBaaS moved from version 1 to version 2. Octavia has become the reference implementation for Neutron LBaaS version 2.
Application delivery, scaling, and availability are considered vital features of any cloud, and load balancing is essential for enabling applications.
Octavia accomplishes its delivery of load balancing services by managing a fleet of virtual machines, containers, or bare metal servers, collectively known as amphorae, which it spins up on demand.
This on-demand, horizontal scaling feature differentiates Octavia from other load balancing solutions, thereby making Octavia truly suited “for the cloud.” In short, Octavia provides a reference implementation of OpenStack LBaaS v2 in a “Cloud-like” load balancing operation in order to manage virtual machines and scale networking infrastructure accordingly.
See the OpenStack documentation for more information.
    https://docs.openstack.org/octavia/ussuri/reference/introduction.html
:::

---

Octavia architecture

![Slide 458 diagram](images/slide_458_image_01.jpg){width="80%"}

::: {.notes}
This diagram provides a visual overview of how the Octavia components interact with OpenStack components to provide load balancer services.
The various drivers (amphora, certificate, compute, and network) allow Octavia to be flexible in the choice of back-end plug-ins.
OpenStack users interact with Octavia through the CLI or UI to perform Octavia LBaaS v2 tasks.
The controller(s) and amphorae communicate over a load balancer network. The load balancer network is a traditional Neutron network to which both the controller and amphorae have access, however it is not associated with any one project. The LB Network is also not part of the undercloud and should not be directly exposed to any OpenStack core components other than the Octavia Controller
Note that originally, the API services for Octavia were provided by Neutron. In v4.0, the API services are handled directly by Octavia.
See the OpenStack documentation for more information.
    https://docs.openstack.org/octavia/ussuri/reference/introduction.html
:::

---

Uses amphorae running HAProxy to provide load balancing
Consists of several sub-components (daemons):
Octavia API: Processes API requests, passing to Octavia worker
Octavia worker: Manages load balancing requests
Health manager: Monitors amphorae to ensure they are up and running, and otherwise healthy Handles failover events if amphorae fail unexpectedly
Provides autoscaling for amphorae
Housekeeping manager: Cleans up stale (deleted) database records, manages the spares pool, and manages amphora certificate rotation
Driver Agent: Receives status and statistics updates from vendor provider drivers
Default provider: amphora
Communicates with amphorae using internal REST API
Requires additional TLS certificates
Creates qdhcp namespace for lb-mgmt-net network

Octavia architecture

459

::: {.notes}
To review what is on the diagram, the controller API daemon must have access to both the LB Network and OpenStack components to coordinate and manage the overall activity of the Octavia load balancing system. The health manager connects to the load balancer network using the o-hm0 interface. You will see that in the lab exercise.
Octavia uses amphorae running HAProxy to provide load balancing. It consists of several sub-components (daemons) such as the Octavia API, which processes API requests and passes them to the Octavia worker.
The Octavia worker manages load balancing requests.
The Health manager monitors amphorae to ensure they are up and running, and otherwise healthy. The Health manager handles failover events if amphorae fail unexpectedly and provides autoscaling for amphorae.
The Housekeeping manager cleans up stale (deleted) database records, manages the spares pool, and manages amphora certificate rotation
The Driver Agent receives status and statistics updates from vendor provider drivers. The Default provider is amphora.
Octavia communicates with amphorae using internal REST API. This requires additional TLS certificates. Octavia also creates qdhcp namespace for lb-mgmt-net network.
See the OpenStack documentation for more information on vendor provider drivers:
    https://docs.openstack.org/octavia/ussuri/admin/providers/index.html
:::

---

Implemented as virtual machines, containers, or bare metal servers
Defined in octavia.conf config file
Created dynamically when you create a load balancer
Default is 1 amphora per load balancer; supports a pool, including autoscaling
Reference implementation
Image: Ubuntu 18.04 LTS (bionic) Running HAProxy
Uses m1.amphora flavor: 1 vCPU, 1GB RAM, 2GB disk 2 interfaces:
IP address in load balancer network, lb-mgmt-net
IP address in tenant (project) network
To reach back-end pool members, depending on how any given load balancing service is deployed by the tenant
One image per Octavia installation

LBAAS amphorae

460

::: {.notes}
Each amphora is implemented as a virtual machine instance. It can also be a container or bare metal server. It requires its own dedicated IP address in the LB network and the tenant network, which is owned by the project. The LB network has a qdhcp namespace, supporting the creation of the amphorae.
Octavia manages load balancers with a pre-built Ubuntu 18.04 LTS image which contains the amphora agent. The amphora agent is a load balancing application that has been seeded with cryptographic certificates through the config drive at startup.
Note that the amphorae use a flavor that yields a small VM. Depending upon your environment, you might need larger flavors.
The amphorae communicate with the controllers in a “trusted” way, using certificates. As a result, users should not have command-line access to the amphorae. There is no need for it. Essentially, the amphorae should be a black box from the users’ perspective.
The individual amphora and the load balancer network, lb-mgmt-net, are owned by the admin project. Only users with the admin role can see the amphorae, for example. Other possible roles are discussed later in this lesson.
The amphorae are monitored by sending heartbeat requests (over UDP) to the controller.
To recap, the amorphae are implemented as virtual machines, containers, or bare metal servers. They are defined in octavia.conf config file and created dynamically when you create a load balancer. The default is one amphora per load balancer. The amorpha supports a pool, including autoscaling and healthcheck monitoring.
- The reference implementation is a Ubuntu 18.04 LTS image.
- It should run the HAProxy service and use the m1.amphora flavor which specifies 1 vCPU, 1GB RAM, and 2GB disk.
- The amorpha should have two network interfaces.
- There will be an IP address in load balancer network, lb-mgmt-net.
- There will also be an IP address in the tenant (or project) network that is used to reach back-end pool members, depending on how any given load balancing service is deployed by the tenant.
There should only be one image per Octavia installation
:::

---

Reference implementation: Ubuntu 18.04 LTS (bionic) with HAProxy
Use diskimage-create.sh utility to create the image
diskimage-create.sh . . .

-a i386 | **amd64** | armhf | ppc64le
-b **haproxy**
-d **bionic**/**8** | <other release id>

(architecture type) (backend type)
(distribution release id)

-i **ubuntu-minimal** | fedora | centos-minimal | rhel
-o **amphora-x64-haproxy** | <filename>
-s **2** | <size in GB>
-t **qcow2** | tar | vhd | raw

(base OS)
(output image file name) (image size)
(image type)

Image must be uploaded to the image repository

Building amphorae image

461

::: {.notes}
Here is key information needed to build an amphorae image.
The reference implementation should be Ubuntu 18.04 LTS with HAProxy.
Use the diskimage-create.sh utility to create the image as shown here.
Finally, be sure to upload the image to the image repository.
For more information, see the OpenStack documentation.
    https://docs.openstack.org/octavia/ussuri/admin/amphora-image-build.html
:::

---

[controller_worker]

# Glance parameters to extract image ID to use for amphora. Only one of
# parameters is needed. Using tags is the recommended way to refer to images. # amp_image_id =
# amp_image_tag =

##

b606eebf-b47b-44f2-8b4b-1672bd00ec93 is the amphora-x64-haproxy image ID

amp_image_tag = amphora
# Optional owner ID used to restrict glance images to one owner ID. # This is a recommended security setting.

##

cb9c614cde114acea70e9d45ca070454 is the admin project

amp_image_owner_id = cb9c614cde114acea70e9d45ca070454
# Networks to attach to the Amphorae examples:
#	- All networks defined in the list will be attached to each amphora #	- One primary network

##

2c57f5ee-d69c-4d7c-acf3-47b05c19fe13 is the lb-mgmt-net network

amp_boot_network_list = 2c57f5ee-d69c-4d7c-acf3-47b05c19fe13
# Security group

##

bc225b7d-310e-4b70-9150-024beece80a0 is the lb-mgmt-sec-grp security group

amp_secgroup_list = bc225b7d-310e-4b70-9150-024beece80a0
# Nova (compute) definitions

##

b1d73041-3c81-4e70-a81f-eb9572391353 is the m1.amphora flavor

amp_flavor_id = b1d73041-3c81-4e70-a81f-eb9572391353 amp_ssh_key_name = octavia_ssh_key

Amphorae configuration

462

::: {.notes}
Here you can see a partial example of the octavia.conf file for LBaaS. Some things to take note of are:
The amphora-x64-haproxy image is the image beginning with b606. It is an Ubuntu 18.04 LTS VM running HAProxy.
The m1.amphora flavor is the image beginning with b1d7. It has 1 vCPU, 1GB RAM, 2GB disk.
The lb-mgmt-net network is the one beginning with 2c57f.
The lb-mgmt-sec-grp security group is the image beginning with bc225. It allows ingress communication for ICMP, SSH, and port 9442.
:::

---

CLI: creating LBAAS topology

![Slide 463 diagram](images/slide_463_image_01.png){width="80%"}

::: {.notes}
Here you can see the steps for creating a load balancer to two front-end HTTP (web) servers: web-server1 and web-server2. The web servers can be created at any point in the process.
You will perform these tasks in the lab exercise:
- openstack loadbalancer create --name lbaas --vip-subnet-id private-subnet will create the load balancer with the name lbaas.
- As the load balancer is created, a virtual IP address (VIP) is assigned to the load balancer.
- One or more amphorae will be provisioned at this time.
- Note that you must wait for the load balancer to be created before proceeding.
- Optionally, you can specify --wait on the openstack loadbalancer create command.
openstack port set <vip_port_id> --security-group lb-sec will assign security group rules to the VIP port of the load balancer. In this case, the security group, lb-sec, was created before this example. You will need to allow ingress traffic on ports 80 and 22.
openstack loadbalancer listener create --name lb-http --protocol HTTP --protocol port 80 lbaas will create a listener for the load balancer with the name lb-http for HTTP requests on port 80.
openstack loadbalancer pool create --name lb_pool --protocol HTTP --listener lb-http --lb-algorithm ROUND_ROBIN will create a load balancer pool that is associated with the lb-http listener. Pool members will be the web servers. The algorithm used to distribute traffic will be round robin. Load balancing can also be done based on the source IP of the request or least connections.
openstack loadbalancer healthmonitor create --name lb-monitor --delay 2 --max-retries 2 --timeout 10 --type PING lb_pool will create a health monitor. This is an optional step. In this case, lb-monitor will ping the pool members every 2 seconds. After 2 failed responses, the member is removed from the pool.
- openstack loadbalancer member create --name member-N - -subnet-id private-subnet --address <web-server-N IP> --protocol-port 80 lb-pool will create members of the load balancer pool.
- The members are the web server instances.
- In this example, there are two web servers, so you will need to do this command twice.
- A common approach is to include autoscaling of the web servers, dynamically adding them to the load balancer pool as they are created.
- This is typically implemented using a Heat template to create the load balancer resources, as well as the autoscaling resources.
:::

---

UI: private network port list

![Slide 464 diagram](images/slide_464_image_01.jpg){width="80%"}

::: {.notes}
Here we can see the Private Network Port List as shown on the UI. Highlights and labels have been added to help you navigate this image.
- As you can see, there are six ports listed.
- Two of them are for the two web servers - web-server-1 and web-server-2.
- The ones labeled “tap interface” and “qr- interface” are for the DHCP and router and are used by Neutron for networking.
- For the load balancer in Octavia, the two allocated ports will have a name that starts with “octavia-lb…” The load balancer VIP can be identified by its attached device which will be “Octavia,” while the amphora instance will be attached to compute:nova.
:::

---

Using the lbaas example:
3 networks:
public (172.)
private (10.)
lb-mgmt-net (192.)
3 instances:
web-server-1 web-server-2
amphora instance with 2 interfaces
10.0.0.27 connects to the private network 192.168.0.9 connects to the lb-mgmt-net network

UI: Network topology

465

![Slide 465 diagram](images/slide_465_image_01.png){width="80%"}

::: {.notes}
This image was merged from two different topology screens from the Dashboard UI. It shows an example network topology with the public (external), private (internal), and lb-mgmt-net (load balancer) networks. 
The web server instances from the public network shown in blue are deployed to the private network shown in orange via router 1.
The load balancer amphora instance is connected to both the private (orange) and load balancer (green) networks as shown in the red box. Note that you must be a member of the admin project (with admin role) to see the amphora instance and lb-mgmt-net resources.
Using the lbaas example, you can see that there are three networks: the public network which starts with 172., the private network which starts with 10., and the load balancer or lb-mgmt-net network which starts with 192.
There are also three instances: web-server-1, web-server-2, and amphora which has two interfaces. 10.0.0.27 connects amphora to the private network and 192.168.0.9 connects amphora to the lb-mgmt-net network.
Again, on the UI, this information will span two different topology diagrams.
:::

---

Amphora instance details

466

::: {.notes}
Here we see an example of a normal openstack server list command for the amphora instance with the ID shown
Key details are highlighted. At the top you see the two IP addresses affiliated with this amphora - lb-mgmt-net and private. The flavor is m1.amphora and the image is amphora-x64-haproxy. Octavia_ssh_key is the key name and the name is listed in red. This amphora is associated with two security groups as shown on the bottom. 
This information can be obtained by issuing the command openstack server show <amphora ID>.
:::

---

Managing the amphora

467

::: {.notes}
Here we see sample outputs from two commands used in managing amphorae.
To display the load balancers, use the command openstack loadbalancer list. In this example, the load balancer lbaas is active using amphora as the provider. When you create the load balancer, it is automatically assigned a VIP. In this case the vip_address is 10.0.0.34.
To see the currently provisioned amphora, use the command openstack 
loadbalancer amphora list. You see the load balancer ID and VIP address when you list the amphora. Note that the load balancer ID is the same ID as seen in the previous command as shown by the blue highlights.
Notice the amphora ID highlighted in yellow. The ID is used as part of the amphora instance name. In this case that would be amphora-0f863e07…
:::

---

Managing the amphora

468

::: {.notes}
Here we see an example output from the openstack loadbalancer amphora show command. The ID in yellow and the loadbalancer_id in blue match the IDs from the previous example.
Key information found here includes:
The compute_id, which is the ID of the amphora instance;
vrrp_ip which is the IP of the interface connecting to the private network;
ha_ip, which is the VIP;
vrrp_port_id, which is the Neutron port associated with the connection to the private network;
vrrp_port_id, which is the Neutron port associated with the VIP;
cached_zone, which is the availability zone (AZ) used;
image_id, which is the ID of the amphora image, amphora-x64-haproxy; and
compute_flavor, which is the ID of the flavor used, m1.amphora, to create the amphora.
:::

---

Display statistics

469

::: {.notes}
To display statistics for a selected amphora, use the command openstack loadbalancer amphora stats show <amphora ID>.
Similarly, you can display the same set of statistics for a load balancer:

openstack loadbalancer stats show <load_balancer>
:::

---

openstack loadbalancer status show lbaas

Display load balancer details

470

::: {.notes}
To display load balancer details, use the command openstack loadbalancer amphora status show lbaas.
The output here shows that the load balancer, lbaas, is active using amphora as the provider.
When you create the load balancer, it is automatically assigned a VIP. In this case, the VIP is 10.0.0.34.
You see the load balancer ID and VIP address when you list the amphora.
:::

---

# Additional Topics {.center}

::: {.notes}
In this next section, we’ll look at additional topics related to Octavia and load balancers not previously covered in the Octavia lesson.
:::

---

Octavia supports the following load balancing protocols, defined for the listener resource:
TCP HTTP HTTPS
TERMINATED_HTTPS UDP
Octavia supports the following load balancing algorithms, defined for the pool resource:
ROUND_ROBIN: request is forwarded to each backend server sequentially; repeats at end of list LEAST_CONNECTIONS: requests are distributed to the backend server with the least number of active connections SOURCE_IP: requests are distributed to backend servers based on the source IP address

Load balancing notes

472

::: {.notes}
Octavia supports the following load balancing protocols, defined for the listener resource: TCP, HTTP, HTTPS, TERMINATED_HTTPS, and UDP.
Octavia also supports the following load balancing algorithms, defined for the pool resource: ROUND_ROBIN, LEAST_CONNECTIONS, and SOURCE_IP.
A ROUND_ROBIN request is forwarded to each backend server sequentially. It will then repeat when it reaches the end of the list.
LEAST_CONNECTIONS requests are distributed to the backend server that has the least number of active connections at a given time.
SOURCE_IP requests are distributed to backend servers based on the source IP address. That is to say, requests from unique source IP addresses are consistently directed to the same instance.
:::

---

Octavia requires several ports to be open Several security groups are used/created:
lb-mgmt-sec-grp, applied to the amphora instances, with rules for: Ingress ICMP
Ingress SSH (port 22)
Ingress TCP port 9443
lb-health-mgr-sec-grp, used by the Health Manager, with rules for:
Ingress UDP port 5555
lb-<load_balancer_ID>, created for each load balancer:
Applied to the VIP port on the load balancer, with rules for the load balancer protocol: Ingress HTTP (port 80)
Ingress port 1025

Security groups

473

::: {.notes}
Octavia requires several ports to be open, and there are several security groups which are used or created.
The lb-mgmt-sec-grp is applied to the amphora instances. It has rules for ingress ICMP, ingress SSH on port 22, and ingress TCP on port 9443.
The lb-health-mgr-sec-grp is used by the Health Manager. It has rules for the ingress UDP on port 5555.
The lb-<load_balancer_ID> is created for each load balancer. It is applied to the VIP port on the load balancer, with rules for the load balancer protocol for ingress HTTP on port 80 and ingress on port 1025.
:::

---

Additional features

Elastic load balancing for amphora
Scale up and down with workload using alarms
Active / Standby (HA) amphorae instances
Supports additional layer 7 (L7) policies and rules, for example:
any client request that matches the L7 rule: “request URI starts with ‘/api’ ” - should be routed to the “api” pool

474

::: {.notes}
Elastic load balancing is the main attraction of Octavia, and is also what enables enterprise use cases.  Elastic load balancing for amphora can scale up and scale down with workload using alarms.
Furthermore, because the workload is handled by VMs, it can provide high availability features via active/standby amphorae instances. Elastic load balancing supports additional layer 7 (or L7) policies and rules. For example, any client request that matches the L7 rule “request URI starts with ‘/api’ ” - should be routed to the “api” pool.
:::

---

# L7 Content Switching {.center}

![Slide 475 diagram](images/slide_475_image_01.png){width="80%"}

::: {.notes}
L7 content switching is a type of load balancing done by setting rules and policies for an incoming request from an application. It is called L7 switching because the device switches requests based on the layer 7 or application layer data.
In this simplified diagram, requests that come through the load balancer are forwarded to an L7 content switching server. Here, the contents of the request can be analyzed and compared with policies and rules. A policy is a collection of rules ANDed together. If the application data matches all rules, then it matches the policy.
The policy also enacts the final action of the request in content switching from the following options:
Block the request
Forward the request to another URL, or
Send the request to a particular member
Unlike traditional load balancer members which are all identical - this example demonstrates that each member is expected to serve a different type of content. If a valid request to fetch images comes in, then requests that match such rules will be forwarded to the member which specializes in serving image files.
More information about L7 content switching can be found in the OpenStack documentation.
    https://docs.openstack.org/octavia/latest/user/guides/l7.html
:::

---

Display default load balancing quotas:

openstack loadbalancer quota defaults show
+	+	+
| Field	| Value |
+	+	+
| load_balancer	| -1	|

| listener
| pool

| -1	|
| -1	|

| health_monitor | -1	|

| member
+	+

| -1	|
+

Display quotas for your project:

openstack loadbalancer quota list
+	+

+	+	+	+	+

| project_id	| load_balancer | listener | pool | health_monitor | member |

+	+	+	+	+

+	+
| <your_project_id>
+	+

| None	| None	| None | None	| None	|
+	+	+	+	+

Load balancing quotas

476

:::: {.columns}

::: {.column width="50%"}
![Slide 476 diagram](images/slide_476_image_01.png){width="90%"}
:::

::: {.column width="50%"}
![Slide 476 diagram](images/slide_476_image_02.png){width="90%"}
:::

::::

::: {.notes}
Here you can see how to access information about load balancing quotas. By default all are set to -1, meaning unlimited resources or no quotas are in effect. To display those default quotas, use the command openstack loadbalancer quota defaults show.
Use the openstack loadbalancer quota set command to quotas. These will define overrides for your project. To display quotas for your project, use the command openstack loadbalancer quota list.
:::

---

Independent of Neutron: Octavia API now serviced as WSGI through Apache
public endpoint in service catalog
Load balancer create enhancements:
Can select the target availability zone (AZ) Can select any supported flavor
Can specify the vendor provider Default provider: amphora
When creating LB from CLI, new, optional, --wait option
openstack loadbalancer create lb_name ... --wait
no longer need to check on the LB status
Supports boot volume for amphora disk image

Updates since rocky

477

::: {.notes}
This slide provides an overview of some key updates/changes to Octavia since the Rocky release.
Independent of Neutron, Octavia API is now serviced as WSGI through Apache. It is a public endpoint in the service catalog.
- The load balancer has undergone some enhancements in more recent versions.
- It can select the target availability zone and any supported flavor.
- It can specify the vendor provider, with the default provider being amphora.
- When creating the load balancer from the CLI, there is a now a --wait flag option.
- To invoke that option, use the command openstack loadbalancer create lb_name ...
- --wait.
- With this option, you no longer need to check on the LB status
 
Finally, releases after Rocky support boot volume for the amphora disk image.
:::

---

Display list of available Octavia providers:
openstack loadbalancer provider list
+	+	+
| name	| description	|
+	+	+
| amphora | The Octavia Amphora driver.	|
| octavia | Deprecated alias of the Octavia Amphora driver. |
+	+	+
Display provider capabilities:
openstack loadbalancer provider capability list amphora
+	+	+
| type	| name	| description
+	+	+

| flavor	|  loadbalancer_topology |

The load balancer topology.

One of:

SINGLE - One amphora per load balancer.

ACTIVE_STANDBY - Two amphora per load balancer.

| flavor	| compute_flavor
| availability_zone | compute_zone

| The compute driver flavor ID.
| The compute availability zone.

| availability_zone | management_network	| The management network ID for the amphora.
| availability_zone | valid_vip_networks	| List of network IDs that are allowed for VIP use.
+	+	+

Octavia providers

478

:::: {.columns}

::: {.column width="50%"}
![Slide 478 diagram](images/slide_478_image_01.png){width="90%"}
:::

::: {.column width="50%"}
![Slide 478 diagram](images/slide_478_image_02.png){width="90%"}
:::

::::

::: {.notes}
Here we see how to access information about the load balancing providers and their capabilities.
To display a list of available Octavia providers, use the command openstack loadbalancer provider list. This example shows that the providers are amphora and octavia.To display the provider capabilities, use the command openstack loadbalancer provider capability list amphora. The capabilities are defined as metadata on a flavor or availability zone.
:::

---

Octavia roles (reference)

479

::: {.notes}
These are the roles available for Octavia. Users must have one of the above roles to access the load-balancer API commands.
In general, the user must have the admin role to create and manage LBaaS resources. The other roles are used to limit access while still allowing necessary commands.
:::

---

SUMMARY

::: {.notes}
We will now wrap up our lessons on Octavia.
:::

---

You should now be able to:
Describe the Octavia architecture
Describe the function and purpose of the amphora Use the CLI to implement a load balancer solution

Summary

481

::: {.notes}
You should now be able to:
    Describe the Octavia architecture,
    Describe the function and purpose of the amphora, and
    Use the CLI to implement a load balancer solution
:::

---

EXERCISES

482

Work through the following lab in the exercise book:
Lab: Load Balancer as a Service (Octavia)

![Slide 482 diagram](images/slide_482_image_01.png){width="80%"}

::: {.notes}
You are now ready to begin Lab 8: Load Balancer as a Service (Octavia). Work through the lab by carefully following each step. You are encouraged to look back to the presentations in this module and earlier modules to refresh your memory and reinforce the concepts. Good luck!
:::

---

